{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "# will use them for creating custom directory iterator\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import json\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import DirectoryIterator, ImageDataGenerator\n",
    "from keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression for splitting by whitespace\n",
    "splitter = re.compile(\"\\s+\")\n",
    "base_path = '/home/ec2-user/GitHub/deepfashion_keras/data/img'\n",
    "create_dict_bboxes_path = '/home/ec2-user/GitHub/deepfashion_keras/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folders():\n",
    "    # Read the relevant annotation file and preprocess it\n",
    "    # Assumed that the annotation files are under '<project folder>/data/anno' path\n",
    "    with open('/home/ec2-user/data/deepfashion/eval/list_eval_partition.txt', 'r') as eval_partition_file:\n",
    "        list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
    "        list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
    "        list_all = [(v[0][4:], v[0].split('/')[1].split('_')[-1], v[1]) for v in list_eval_partition]\n",
    "\n",
    "    # Put each image into the relevant folder in train/test/validation folder\n",
    "    for element in list_all:\n",
    "        if not os.path.exists(os.path.join(base_path, element[2])):\n",
    "            os.mkdir(os.path.join(base_path, element[2]))\n",
    "        if not os.path.exists(os.path.join(os.path.join(base_path, element[2]), element[1])):\n",
    "            os.mkdir(os.path.join(os.path.join(base_path, element[2]), element[1]))\n",
    "        if not os.path.exists(os.path.join(os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1])),\n",
    "                              element[0].split('/')[0])):\n",
    "            os.mkdir(os.path.join(os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1])),\n",
    "                     element[0].split('/')[0]))\n",
    "        shutil.move(os.path.join(base_path, element[0]),\n",
    "                    os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1]), element[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need to run once after unzipping images\n",
    "process_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_bboxes(list_all, n = None,  split='train'):\n",
    "    if n:\n",
    "        list_all = list_all[:n]\n",
    "    lst = [(line[0], line[1], line[3], line[2]) for line in list_all if line[2] == split]\n",
    "    lst = [(\"\".join(line[0].split('/')[0] + '/' + line[3] + '/' + line[1] + line[0][3:]), line[1], line[2]) for line in lst]\n",
    "    lst_shape = [cv2.imread(create_dict_bboxes_path + line[0]).shape for line in lst]\n",
    "    lst_norm = [(line[0], line[1], (round(line[2][0] / shape[1], 2), round(line[2][1] / shape[0], 2)\n",
    "                                    , round(line[2][2] / shape[1], 2), round(line[2][3] / shape[0], 2)),shape) for line, shape in zip(lst, lst_shape)]\n",
    "    dict_ = {\"/\".join(line[0].split('/')[2:]): {'x1': line[2][0], 'y1': line[2][1]\n",
    "                                                , 'x2': line[2][2], 'y2': line[2][3], 'shape': line[3]} for line in lst_norm}\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_bboxes(n = None):\n",
    "    with open('/home/ec2-user/data/deepfashion/anno/list_category_img.txt', 'r') as category_img_file, \\\n",
    "            open('/home/ec2-user/data/deepfashion/eval/list_eval_partition.txt', 'r') as eval_partition_file, \\\n",
    "            open('/home/ec2-user/data/deepfashion/anno/list_bbox.txt', 'r') as bbox_file:\n",
    "        list_category_img = [line.rstrip('\\n') for line in category_img_file][2:]\n",
    "        list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
    "        list_bbox = [line.rstrip('\\n') for line in bbox_file][2:]\n",
    "\n",
    "        list_category_img = [splitter.split(line) for line in list_category_img]\n",
    "        list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
    "        list_bbox = [splitter.split(line) for line in list_bbox]\n",
    "\n",
    "        list_all = [(k[0], k[0].split('/')[1].split('_')[-1], v[1], (int(b[1]), int(b[2]), int(b[3]), int(b[4])))\n",
    "                    for k, v, b in zip(list_category_img, list_eval_partition, list_bbox)]\n",
    "        \n",
    "        list_all.sort(key=lambda x: x[1])\n",
    "\n",
    "        dict_train = create_dict_bboxes(list_all, n)\n",
    "        dict_val = create_dict_bboxes(list_all, n, split='val')\n",
    "        dict_test = create_dict_bboxes(list_all, n, split='test')\n",
    "\n",
    "        return dict_train, dict_val, dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectoryIteratorWithBoundingBoxes(DirectoryIterator):\n",
    "    def __init__(self, directory, image_data_generator, bounding_boxes: dict = None, target_size=(256, 256),\n",
    "                 color_mode: str = 'rgb', classes=None, class_mode: str = 'categorical', batch_size: int = 32,\n",
    "                 shuffle: bool = True, seed=None, data_format=None, save_to_dir=None,\n",
    "                 save_prefix: str = '', save_format: str = 'jpeg', follow_links: bool = False):\n",
    "        super().__init__(directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size,\n",
    "                         shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links)\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        locations = np.zeros((len(batch_x),) + (4,), dtype=K.floatx())\n",
    "\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname = self.filenames[j]\n",
    "            img = image.load_img(os.path.join(self.directory, fname),\n",
    "                                 grayscale=grayscale,\n",
    "                                 target_size=self.target_size)\n",
    "            x = image.img_to_array(img, data_format=self.data_format)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "\n",
    "            if self.bounding_boxes is not None:\n",
    "                bounding_box = self.bounding_boxes[fname]\n",
    "                locations[i] = np.asarray(\n",
    "                    [bounding_box['x1'], bounding_box['y1'], bounding_box['x2'], bounding_box['y2']],\n",
    "                    dtype=K.floatx())\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        # build batch of labels\n",
    "        if self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), 46), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "\n",
    "        if self.bounding_boxes is not None:\n",
    "            return batch_x, [batch_y, locations]\n",
    "        else:\n",
    "            return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0921 05:08:02.671348 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0921 05:08:02.697949 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0921 05:08:02.707729 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0921 05:08:02.734481 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0921 05:08:02.735152 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0921 05:08:02.930615 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0921 05:08:02.998501 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0921 05:08:13.536563 140664968615744 deprecation_wrapper.py:119] From /home/ec2-user/miniconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "for layer in model_resnet.layers[:-12]:\n",
    "    # 6 - 12 - 18 have been tried. 12 is the best.\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = model_resnet.output\n",
    "x = Dense(512, activation='elu', kernel_regularizer=l2(0.001))(x)\n",
    "y = Dense(46, activation='softmax', name='img')(x)\n",
    "\n",
    "x_bbox = model_resnet.output\n",
    "x_bbox = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x_bbox)\n",
    "x_bbox = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x_bbox)\n",
    "bbox = Dense(4, kernel_initializer='normal', name='bbox')(x_bbox)\n",
    "\n",
    "final_model = Model(inputs=model_resnet.input,\n",
    "                    outputs=[y, bbox])\n",
    "\n",
    "opt = SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "\n",
    "final_model.compile(optimizer=opt,\n",
    "                    loss={'img': 'categorical_crossentropy',\n",
    "                          'bbox': 'mean_squared_error'},\n",
    "                    metrics={'img': ['accuracy', 'top_k_categorical_accuracy'], # default: top-5\n",
    "                             'bbox': ['mse']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train, dict_val, dict_test = get_dict_bboxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_train = json.dumps(dict_train)\n",
    "f = open(\"/home/ec2-user/GitHub/deepfashion_keras/data/img_dicts/dict_train.json\",\"w\")\n",
    "f.write(json_train)\n",
    "f.close()\n",
    "\n",
    "json_val = json.dumps(dict_val)\n",
    "f = open(\"/home/ec2-user/GitHub/deepfashion_keras/data/img_dicts/dict_val.json\",\"w\")\n",
    "f.write(json_val)\n",
    "f.close()\n",
    "\n",
    "json_test = json.dumps(dict_test)\n",
    "f = open(\"/home/ec2-user/GitHub/deepfashion_keras/data/img_dicts/dict_test.json\",\"w\")\n",
    "f.write(json_test)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209222 images belonging to 46 classes.\n",
      "Found 40000 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"/home/ec2-user/GitHub/deepfashion_keras/data/img/train\"\n",
    "val_dir = \"/home/ec2-user/GitHub/deepfashion_keras/data/img/val\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=30.,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   preprocessing_function=preprocess_input())\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input())\n",
    "\n",
    "train_iterator = DirectoryIteratorWithBoundingBoxes(train_dir, train_datagen, bounding_boxes=dict_train, target_size=(200, 200))\n",
    "val_iterator = DirectoryIteratorWithBoundingBoxes(val_dir, val_datagen, bounding_boxes=dict_val,target_size=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               patience=12,\n",
    "                               factor=0.5,\n",
    "                               verbose=1)\n",
    "tensorboard = TensorBoard(log_dir='/home/ec2-user/GitHub/deepfashion_keras/tb_logs')\n",
    "early_stopper = EarlyStopping(monitor='val_loss',\n",
    "                              patience=30,\n",
    "                              verbose=1)\n",
    "performance_log = CSVLogger('/home/ec2-user/GitHub/deepfashion_keras/csv_logs/model_10_epoch_log.csv',separator=',',append=False)\n",
    "checkpoint = ModelCheckpoint('/home/ec2-user/GitHub/deepfashion_keras/models/model_10_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generator(iterator):\n",
    "    while True:\n",
    "        batch_x, batch_y = iterator.next()\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "final_model.fit_generator(custom_generator(train_iterator),\n",
    "                          steps_per_epoch= 6538, # 209222 train records / batch size of 32\n",
    "                          epochs=10, \n",
    "                          validation_data=custom_generator(test_iterator),\n",
    "                          validation_steps= 1250, # 40000 val records/ batch size of 32\n",
    "                          verbose=2,\n",
    "                          shuffle=True,\n",
    "                          callbacks=[lr_reducer, checkpoint, early_stopper, tensorboard, performance_log],\n",
    "                          workers=12\n",
    "                         ,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 images belonging to 46 classes.\n",
      "Multi target loss: 4.740625619888306\n",
      "Image loss: 2.8252378702163696\n",
      "Bounding boxes loss: 0.07381153516471387\n",
      "Image accuracy: 0.278125\n",
      "Top-5 image accuracy: 0.578125\n",
      "Bounding boxes error: 0.07381153516471387\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input())\n",
    "\n",
    "test_dir = \"/home/ec2-user/GitHub/deepfashion_keras/data/img/test\"\n",
    "\n",
    "\n",
    "test_iterator = DirectoryIteratorWithBoundingBoxes(test_dir, test_datagen, bounding_boxes=dict_test, target_size=(200, 200))\n",
    "scores = final_model.evaluate_generator(custom_generator(test_iterator), steps=10)\n",
    "\n",
    "print('Multi target loss: ' + str(scores[0]))\n",
    "print('Image loss: ' + str(scores[1]))\n",
    "print('Bounding boxes loss: ' + str(scores[2]))\n",
    "print('Image accuracy: ' + str(scores[3]))\n",
    "print('Top-5 image accuracy: ' + str(scores[4]))\n",
    "print('Bounding boxes error: ' + str(scores[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
